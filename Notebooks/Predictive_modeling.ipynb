{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86baed12",
   "metadata": {},
   "source": [
    "# <center><font color='#F1B03D'>**Revenue Intelligence Enhancement for BrokerChooser - Predictive Modeling**</font></center>\n",
    "### <center><font color='#F1B03D'>Central European University, 2024-2025</font></center>\n",
    "### <center><font color='#F1B03D'>CEU Capstone Project</font></center>\n",
    "\n",
    "### <left><font color='#F1B03D'>Author: Péter Bence Török (torokpe@gmail.com)</font></left>\n",
    "### <left><font color='#F1B03D'>BrokerChooser Contact Person: Zoltán Molnár (zoltan.molnar@brokerchooser.com)</font></left>\n",
    "\n",
    "---\n",
    "<p style=\"font-size:22px;\"> This notebook focuses on the predictive modeling and classification phase of the project. It uses the previously cleaned and feature-engineered dataset to train and evaluate multiple classification models, including logistic regression (in five variations), LASSO, Random Forest, and XGBoost. The models are assessed using cross-validated performance metrics such as AUC and RMSE, and custom misclassification costs are applied to find optimal probability thresholds. The goal of this notebook is to identify the most effective model for predicting revenue-generating sessions based on user behavior and session attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate each logistic regression model\n",
    "for name, features in Logits.items():\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_sampled[features], y_sampled)\n",
    "    logit_models[name] = model\n",
    "\n",
    "    # Cross-validated AUC\n",
    "    auc = cross_val_score(model, X_sampled[features], y_sampled, \n",
    "                          cv=5, scoring='roc_auc', n_jobs=-1).mean()\n",
    "\n",
    "    # Cross-validated RMSE from Brier score\n",
    "    brier_scores = cross_val_score(\n",
    "        model, X_sampled[features], y_sampled, \n",
    "        cv=5,\n",
    "        scoring=make_scorer(brier_score_loss, needs_proba=True),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = np.mean(np.sqrt(brier_scores))  # Convert Brier to RMSE\n",
    "\n",
    "    # Add result to collector\n",
    "    results.add_model(name, len(features), auc, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0117901",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Lasso + Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularization strength values (Cs are the inverse of λ)\n",
    "lambdas = 10 ** np.arange(-1, -4.01, -1 / 3)  # λ from 0.1 to 0.0001\n",
    "n_obs = int(len(y_sampled) * 4 / 5)  # Approximate train set size for scaling Cs\n",
    "C_values = [1 / (l * n_obs) for l in lambdas]\n",
    "\n",
    "# Fit logistic regression with L1 regularization using cross-validation\n",
    "log_lasso_model = LogisticRegressionCV(\n",
    "    Cs=C_values,\n",
    "    penalty=\"l1\",\n",
    "    cv=5,\n",
    "    solver=\"liblinear\",\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Compute performance metrics\n",
    "log_lasso_model.fit(X_sampled, y_sampled)\n",
    "y_pred_proba = log_lasso_model.predict_proba(X_sampled)[:, 1]\n",
    "avg_auc = roc_auc_score(y_sampled, y_pred_proba)\n",
    "avg_rmse = np.sqrt(brier_score_loss(y_sampled, y_pred_proba))\n",
    "\n",
    "# Store results\n",
    "results.add_model(\n",
    "    name=\"LASSO\",\n",
    "    num_var=X_sampled.shape[1],\n",
    "    auc=avg_auc,\n",
    "    rmse=avg_rmse\n",
    ")\n",
    "\n",
    "print(f\"Logit + LASSO - RMSE: {avg_rmse:.4f}, AUC: {avg_auc:.4f}\")\n",
    "\n",
    "# Storing model\n",
    "logit_models[\"Logit + LASSO\"] = log_lasso_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7408c68",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Probability Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_features': [5, 10, 20],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [10, 21, 30]\n",
    "}\n",
    "\n",
    "# Store feature names\n",
    "rf_feature_names = list(X_sampled.columns)\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV with ROC AUC and Brier score as scoring metrics\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=['roc_auc', 'neg_brier_score'],\n",
    "    refit='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2  # Verbose level added\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_sampled, y_sampled)\n",
    "\n",
    "# Extract best parameters and calculate RMSE and AUC\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "cv_rmse = np.mean([\n",
    "    np.sqrt(-1 * grid_search.cv_results_[f\"split{i}_test_neg_brier_score\"])\n",
    "    for i in range(5)\n",
    "])\n",
    "\n",
    "# Calculate average CV AUC\n",
    "cv_auc = np.mean([\n",
    "    grid_search.cv_results_[f\"split{i}_test_roc_auc\"]\n",
    "    for i in range(5)\n",
    "])\n",
    "\n",
    "# Save results\n",
    "results.add_model(\n",
    "    name=\"Random Forest\",\n",
    "    num_var=X_sampled.shape[1],\n",
    "    auc=cv_auc,\n",
    "    rmse=cv_rmse\n",
    ")\n",
    "\n",
    "# Best performing model\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "print(f\"Random Forest - CV RMSE: {cv_rmse:.4f}, CV AUC: {cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f031b",
   "metadata": {},
   "source": [
    "## 2.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37952ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define scoring\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'neg_brier_score': 'neg_brier_score'\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "xgb_grid.fit(X_sampled, y_sampled)\n",
    "\n",
    "# Extract best model and evakuate performance\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "cv_auc = np.mean([\n",
    "    xgb_grid.cv_results_[f\"split{i}_test_roc_auc\"]\n",
    "    for i in range(5)\n",
    "])\n",
    "\n",
    "cv_rmse = np.mean([\n",
    "    np.sqrt(-1 * xgb_grid.cv_results_[f\"split{i}_test_neg_brier_score\"])\n",
    "    for i in range(5)\n",
    "])\n",
    "\n",
    "# Save results\n",
    "results.add_model(\n",
    "    name=\"XGBoost\",\n",
    "    num_var=X_sampled.shape[1],\n",
    "    auc=cv_auc,\n",
    "    rmse=cv_rmse\n",
    ")\n",
    "\n",
    "print(f\"XGBoost - CV RMSE: {cv_rmse:.4f}, CV AUC: {cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46000a",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining costs for false positives and false negatives clasifications\n",
    "FP_cost = 1\n",
    "FN_cost = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best classification threshold based on custom misclassification costs\n",
    "def find_optimal_threshold(model, X, y_true, FP_cost=1, FN_cost=20):\n",
    "    # Get predicted probabilities for the positive class\n",
    "    y_probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Define a range of threshold values from 0 to 1\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = []  # List to store the total cost at each threshold\n",
    "\n",
    "    # Loop through all thresholds and compute the total cost\n",
    "    for t in thresholds:\n",
    "        # Convert probabilities to binary predictions based on threshold\n",
    "        y_pred = (y_probs >= t).astype(int)\n",
    "\n",
    "        # Get confusion matrix components\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        # Calculate total cost using the specified FP and FN costs\n",
    "        total_cost = FP_cost * fp + FN_cost * fn\n",
    "        costs.append(total_cost)\n",
    "\n",
    "    # Find the threshold that minimizes the total cost\n",
    "    best_index = np.argmin(costs)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    min_cost = costs[best_index]\n",
    "\n",
    "    # Return the best threshold, its cost, and the full cost curve for plotting\n",
    "    return best_threshold, min_cost, thresholds, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "90b0f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds are the same (np.linspace(0, 1, 100)), so we can reuse them\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# Run the function for each model\n",
    "threshold_logit, cost_logit, _, costs_logit = find_optimal_threshold(logit_models[\"Logit_M5\"], X_test, y_test)\n",
    "threshold_lasso, cost_lasso, _, costs_lasso = find_optimal_threshold(log_lasso_model, X_test, y_test)\n",
    "threshold_rf, cost_rf, _, costs_rf = find_optimal_threshold(best_random_forest, X_test, y_test)\n",
    "threshold_xgb, cost_xgb, _, costs_xgb = find_optimal_threshold(best_xgb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8309b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(thresholds, costs_logit, label=f'Logit M5 (min cost: {cost_logit:.0f})', linewidth=2)\n",
    "plt.plot(thresholds, costs_lasso, label=f'Lasso (min cost: {cost_lasso:.0f})', linewidth=2)\n",
    "plt.plot(thresholds, costs_rf, label=f'Random Forest (min cost: {cost_rf:.0f})', linewidth=2)\n",
    "plt.plot(thresholds, costs_xgb, label=f'XGBoost (min cost: {cost_xgb:.0f})', linewidth=2)\n",
    "\n",
    "# Mark best thresholds with vertical lines\n",
    "plt.axvline(x=threshold_logit, color='blue', linestyle='--', alpha=0.1)\n",
    "plt.axvline(x=threshold_lasso, color='orange', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=threshold_rf, color='green', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=threshold_xgb, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Final plot formatting\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Total Misclassification Cost')\n",
    "plt.title('Cost Curves for Different Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC values\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(fpr, tpr, color='#EFC64A', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.fill_between(fpr, tpr, alpha=0.2, color='#81C6C7')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Random Guess')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
